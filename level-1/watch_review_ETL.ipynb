{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-5lIkJR_xdS"
      },
      "source": [
        "import os\n",
        "# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n",
        "# For example:\n",
        "spark_version = 'spark-3.0.3'\n",
        "os.environ['SPARK_VERSION']=spark_version\n",
        "\n",
        "# Install Spark and Java\n",
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "# Set Environment Variables\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
        "\n",
        "# Start a SparkSession\n",
        "import findspark\n",
        "findspark.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9-72LgjD_1b6"
      },
      "source": [
        "!wget https://jdbc.postgresql.org/download/postgresql-42.2.9.jar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4bfEfS8_31Y"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"amazonETL\").config(\"spark.driver.extraClassPath\",\"/content/postgresql-42.2.9.jar\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fc0cLdV0_6o1"
      },
      "source": [
        "# Read in data from S3 Buckets\n",
        "from pyspark import SparkFiles\n",
        "url=\"https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Watches_v1_00.tsv.gz\"\n",
        "spark.sparkContext.addFile(url)\n",
        "user_data_df = spark.read.csv(SparkFiles.get(\"amazon_reviews_us_Watches_v1_00.tsv.gz\"), sep=\"\\t\", header=True, inferSchema=True, timestampFormat=\"yyyy/mm/dd\")\n",
        "\n",
        "# Show DataFrame\n",
        "user_data_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_QoJC5DAE13"
      },
      "source": [
        "number_of_rows = user_data_df.select(\"customer_id\").count()\n",
        "print(f\"The number of rows in the above dataset is {number_of_rows}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4S-rXZ8AKaQ"
      },
      "source": [
        "# Drop NaN Values\n",
        "user_data_df = user_data_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dLDqU40ALNT"
      },
      "source": [
        "# Create dataframe to match schema\n",
        "\n",
        "#   review_id_table (\n",
        "#   review_id TEXT PRIMARY KEY NOT NULL,\n",
        "#   customer_id INTEGER,\n",
        "#   product_id TEXT,\n",
        "#   product_parent INTEGER,\n",
        "#   review_date DATE\n",
        "# );\n",
        "\n",
        "review_id_df = user_data_df.select([\"review_id\",\"customer_id\",\"product_id\",\"product_parent\",\"review_date\"])\n",
        "review_id_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCEwXszNAPND"
      },
      "source": [
        "# Changing review_date to DateType\n",
        "from pyspark.sql.types import DateType\n",
        "review_id_df = review_id_df.withColumn(\"review_date\", review_id_df[\"review_date\"].cast(DateType()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC_BtDWNAPJv"
      },
      "source": [
        "# Confirm datatypes\n",
        "review_id_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeU1krz8APE6"
      },
      "source": [
        "#   products (\n",
        "#   product_id TEXT PRIMARY KEY NOT NULL UNIQUE,\n",
        "#   product_title TEXT\n",
        "# );\n",
        "\n",
        "product_id_df = user_data_df.select([\"product_id\",\"product_title\"]).distinct()\n",
        "product_id_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uo88iPiOAPCa"
      },
      "source": [
        "product_id_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chCYwuf4AO_c"
      },
      "source": [
        "# CREATE TABLE customers (\n",
        "#   customer_id INT PRIMARY KEY NOT NULL UNIQUE,\n",
        "#   customer_count INT\n",
        "# );\n",
        "\n",
        "customer_id_df = user_data_df.groupby(\"customer_id\").agg({\"customer_id\": \"count\"}).withColumnRenamed(\"count(customer_id)\", \"customer_count\")\n",
        "customer_id_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnfSJ_48AO9F"
      },
      "source": [
        "customer_id_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLiCwrGwAO3X"
      },
      "source": [
        "# CREATE TABLE vine_table (\n",
        "#   review_id TEXT PRIMARY KEY,\n",
        "#   star_rating INTEGER,\n",
        "#   helpful_votes INTEGER,\n",
        "#   total_votes INTEGER,\n",
        "#   vine TEXT\n",
        "# );\n",
        "\n",
        "vine_df = user_data_df.select([\"review_id\",\"star_rating\",\"helpful_votes\",\"total_votes\",\"vine\"])\n",
        "vine_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US701SANAO0T"
      },
      "source": [
        "# Convert star_rating from string type to integer\n",
        "from pyspark.sql.types import IntegerType\n",
        "vine_df = vine_df.withColumn(\"star_rating\", vine_df[\"star_rating\"].cast(IntegerType()))\n",
        "vine_df.show(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2kbBz8eAjna"
      },
      "source": [
        "vine_df.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4xoWvJbAjlG"
      },
      "source": [
        "mode = \"append\"\n",
        "jdbc_url=\"jdbc:postgresql://amazondb.cu4hemf5gle4.us-east-2.rds.amazonaws.com:5432/watches_reviewDB\"\n",
        "config = {\"user\":\"postgres\", \n",
        "          \"password\": \"postgres1234\", \n",
        "          \"driver\":\"org.postgresql.Driver\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UphKR-MAAjiS"
      },
      "source": [
        "# Write DataFrame to table in RDS\n",
        "\n",
        "review_id_df.write.jdbc(url=jdbc_url, table='review_id_table', mode=mode, properties=config)\n",
        "product_id_df.write.jdbc(url=jdbc_url, table='products', mode=mode, properties=config)\n",
        "customer_id_df.write.jdbc(url=jdbc_url, table='customers', mode=mode, properties=config)\n",
        "vine_df.write.jdbc(url=jdbc_url, table='vine_table', mode=mode, properties=config)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}